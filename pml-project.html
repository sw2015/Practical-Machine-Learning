<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Project for Practical Machine Learning</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Project for Practical Machine Learning</h1>

<h2>Background</h2>

<p>Six people perform barbell lifts. The lifts are classified in 5 different ways. </p>

<ol>
<li>exactly according to the specification (Class A)</li>
<li>throwing the elbows to the front (Class B)</li>
<li>lifting the dumbbell only halfway (Class C)</li>
<li>lowering the dumbbell only halfway (Class D)</li>
<li>throwing the hips to the front (Class E)</li>
</ol>

<p>More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset). </p>

<h2>Data Preparation</h2>

<p>Download the data to the local computer: </p>

<p>training set: pml-training.csv</p>

<p>testing set: pml-testing.csv</p>

<h2>Load Libraries</h2>

<pre><code>
library(ggplot2)
library(caret)
library(randomForest)

</code></pre>

<h2>Clean Data</h2>

<p>Load to R by replacing empty entries with NA’s</p>

<pre><code>training &lt;- read.csv(&quot;~/Desktop/machine learning/pml-training.csv&quot;, na.strings = c(&quot;&quot;, &quot;NA&quot;))

testing &lt;- read.csv(&quot;~/Desktop/machine learning/pml-testing.csv&quot;, na.strings = c(&quot;&quot;, &quot;NA&quot;))

</code></pre>

<p>Remove empty entries and NA’s</p>

<pre><code>
training &lt;- training[, colSums(is.na(training)) == 0]

testing &lt;- testing[, colSums(is.na(testing)) == 0]

</code></pre>

<p>Remove the first seven columns in the training set and testing set which are unrelated to the activities:</p>

<pre><code>
training &lt;- training[-c(1:7)]

testing &lt;- testing[-c(1:7)]

</code></pre>

<p>There are a total of 53 columns after cleaning</p>

<pre><code>
colnames(training)

 [1] &quot;roll_belt&quot;            &quot;pitch_belt&quot;           &quot;yaw_belt&quot;            
 [4] &quot;total_accel_belt&quot;     &quot;gyros_belt_x&quot;         &quot;gyros_belt_y&quot;        
 [7] &quot;gyros_belt_z&quot;         &quot;accel_belt_x&quot;         &quot;accel_belt_y&quot;        
[10] &quot;accel_belt_z&quot;         &quot;magnet_belt_x&quot;        &quot;magnet_belt_y&quot;       
[13] &quot;magnet_belt_z&quot;        &quot;roll_arm&quot;             &quot;pitch_arm&quot;           
[16] &quot;yaw_arm&quot;              &quot;total_accel_arm&quot;      &quot;gyros_arm_x&quot;         
[19] &quot;gyros_arm_y&quot;          &quot;gyros_arm_z&quot;          &quot;accel_arm_x&quot;         
[22] &quot;accel_arm_y&quot;          &quot;accel_arm_z&quot;          &quot;magnet_arm_x&quot;        
[25] &quot;magnet_arm_y&quot;         &quot;magnet_arm_z&quot;         &quot;roll_dumbbell&quot;       
[28] &quot;pitch_dumbbell&quot;       &quot;yaw_dumbbell&quot;         &quot;total_accel_dumbbell&quot;
[31] &quot;gyros_dumbbell_x&quot;     &quot;gyros_dumbbell_y&quot;     &quot;gyros_dumbbell_z&quot;    
[34] &quot;accel_dumbbell_x&quot;     &quot;accel_dumbbell_y&quot;     &quot;accel_dumbbell_z&quot;    
[37] &quot;magnet_dumbbell_x&quot;    &quot;magnet_dumbbell_y&quot;    &quot;magnet_dumbbell_z&quot;   
[40] &quot;roll_forearm&quot;         &quot;pitch_forearm&quot;        &quot;yaw_forearm&quot;         
[43] &quot;total_accel_forearm&quot;  &quot;gyros_forearm_x&quot;      &quot;gyros_forearm_y&quot;     
[46] &quot;gyros_forearm_z&quot;      &quot;accel_forearm_x&quot;      &quot;accel_forearm_y&quot;     
[49] &quot;accel_forearm_z&quot;      &quot;magnet_forearm_x&quot;     &quot;magnet_forearm_y&quot;    
[52] &quot;magnet_forearm_z&quot;     &quot;classe&quot; 

</code></pre>

<h2>Data Partition</h2>

<pre><code>
inTrain &lt;- createDataPartition(training$classe, p = 0.7, list = FALSE)

train &lt;- training[inTrain, ]

test &lt;-  training[-inTrain, ]

</code></pre>

<h2>Model Build</h2>

<p>Use random forest algorithm</p>

<pre><code>modelFit &lt;- train(classe ~ ., data = train, method = &quot;rf&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5))

</code></pre>

<p>Here I am using k-fold cross-validation to sample the data with 5 folds instead of bootstrapping</p>

<h2>Data Prediction</h2>

<p>Use the test data that we sampled from training set:</p>

<pre><code>pred &lt;- predict(modelFit, test)

</code></pre>

<h2>Out of Sample Error</h2>

<p>Use Confusion Matrix to compute out of sample error:</p>

<p>1 - 0.9932 = 0.0068 = 0.68%</p>

<pre><code>
confusionMatrix(test$classe, pred)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1673    0    1    0    0
         B    9 1128    2    0    0
         C    0    8 1015    3    0
         D    0    0   11  952    1
         E    0    1    3    1 1077

Overall Statistics

               Accuracy : 0.9932          
                 95% CI : (0.9908, 0.9951)
    No Information Rate : 0.2858          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       

                  Kappa : 0.9914          
 Mcnemar&#39;s Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9946   0.9921   0.9835   0.9958   0.9991
Specificity            0.9998   0.9977   0.9977   0.9976   0.9990
Pos Pred Value         0.9994   0.9903   0.9893   0.9876   0.9954
Neg Pred Value         0.9979   0.9981   0.9965   0.9992   0.9998
Prevalence             0.2858   0.1932   0.1754   0.1624   0.1832
Detection Rate         0.2843   0.1917   0.1725   0.1618   0.1830
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9972   0.9949   0.9906   0.9967   0.9990

</code></pre>

<p>Note that accuracy is 0.9911 in the training data with 5-fold cross-validation as mtry = 27 is used:</p>

<pre><code>
modelFit
Random Forest 

13737 samples
   52 predictor
    5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 

No pre-processing
Resampling: Cross-Validated (5 fold) 

Summary of sample sizes: 10990, 10991, 10989, 10989, 10989 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9907548  0.9883033  0.002339049  0.002960321
  27    0.9911187  0.9887649  0.000800172  0.001011936
  52    0.9857319  0.9819513  0.004952988  0.006264063

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27.

</code></pre>

<h2>Test Data</h2>

<pre><code>
predict(modelFit, newdata = testing)

[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E

</code></pre>

</body>

</html>
